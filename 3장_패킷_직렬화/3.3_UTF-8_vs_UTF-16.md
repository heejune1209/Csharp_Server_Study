UNICODE & ENCODING

UNICODE와 ENCODING는 결국 문자를 어떻게 컴퓨터로 표현할 것인가에 대한 문제 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/828c34f6-03e8-4fe2-b412-728468239927/Untitled.png)

1바이트(ASCII) 코드에서 2바이트로 늘린 버전이 UNICODE

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/21d172d1-7e13-4049-8246-8186f38abc77/Untitled.png)

UNICODE는 최대 3바이트 범위까지 ⇒ 나중에 더 늘어날 수도 있다.

유니코드는 말 그대로 표현하고 싶은 문자와 값을 맵핑

인코딩 → 유니코드를 컴퓨터에게 어떻게 알려줄 것인지 결정

UTF-8

영문 : 1바이트(0~127)

한글 : 3바이트로 표현이 된다.

⇒ 영문이 많이 사용하는 경우는 UTF-8이 더 효율적이다.

UTF-16

BMP영역(바이트로 표현할 수 있는 최대 문자 영역) ⇒ 2바이트

BMP영역을 벗어나면 ⇒ 4바이트

기본적으로 C#은 UTF-16을 기본적으로 사용한다.

다만 파일 입출력은 UTF-8을 사용 

char : 2바이트(C#)

char : 1바이트(C++)

2바이트로 표현하는 것이 조금 더 합리적이라는 의유는 대부분이 2바이트 안에 들어가기 때문이다.

영문, 한글, 중문 모두 2바이트이다.

영문에서 봤을 때는 UTF-16이 조금 비효율적이지만 일반적인 관점에서 봤을 때는 효율적이다.

언리얼 엔진에서도 기본으로 UTF-16을 지원한다.

결국 서버로 데이터를 주고 받을 때는 문자라는 개념이 굉장히 모호하다.

유니코드는 공통적이지만 어떤 식으로 인코딩해서 데이터로 들고 있을지는 갈린다 

⇒ 따라서 UTF-8로 할지 UTF-16으로 서로 통신을 할지 결정을 해줘야 한다.